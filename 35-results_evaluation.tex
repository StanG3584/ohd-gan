Results: Model validation and data evaluation
To asses the solution to a generative modelling problem, it is necessary to validate the model obtained, and subsequently to verify its output. GANs aim to approximate a data distribution PP​, using a parameterized model distribution QQ​ (Borji 2018). Thus, in evaluating the model, the goal is to validate that the learning process has led to a sufficiently close approximation. Approaches to evaluation can be categorized as either quantitative or qualitative. 
Qualitative evaluation
The qualitative evaluation approaches found in the literature are mainly preference judgement, discrimination tasks, clinician evaluation (Borji 2018). Participants, such as medical professionals, discriminate between real and synthetic instances (Choi 2017b), or asked to rank the quality of real and synthetic samples on a numerical scale and their significance is determined with a Mann-Whitney U test (Beaulieu-Jones 2019). Similarly, visual inspection of statistics or projections of the data can help get a better understanding of model behaviour (Beaulieu-Jones 2019, Che 2017), but are often weak indicators of model performance without more objective  metrics (Jackson 2019). 
Quantitative evaluation
Comparing distributions
Numerous statistical metrics have been proposed or explored to compare the distributions of real and synthetic data (Borji 2018). We present here those employed in the publication included in the review in Tab. 2.

\begin{table}
    \caption{Metrics employed to validate trained models based on the comparison of distributions.\label{tab:evaldist}} 
        
    \begin{tabular}{@{} p{0.2\textwidth} p{0.2\textwidth} p{0.2\textwidth} p{0.2\textwidth} @{}}\toprule
        
        Metric & Description & Example & References\\\midrule
        
        Kullback-Leibler (KL) divergence & Compares the distributions isolated features by measuring the similarity of their marginal probability mass functions (PMF). & - &
        \cite{Goncalves2020}\\
        
        Maximum Mean Discrepancy (MMD) & 
        Checks the dissimilarity between the real and synthetic probability distributions using samples drawn independently from each other. & - &
        \cite{esteban2017real}\\
        
        2-sample test (2-ST) & Answers whether two samples, the real and synthetic, originate from the same distribution through the use of a statistical test. & 
        Kolmogorov-Smirnov (KS) & 
        \cite{Fisher2019,Baowaly2019}\\
        
        Distribution of Reconstruction Error & 
        Determine if the samples in the synthetic set are more similar to those in the training set than those in the testing set. & Nearest-neighbor &
        \cite{esteban2017real}\\
        
        Latent projection distribution & 
        Compares the distribution of real and synthetic samples projected back into the latent space & Mean of the variance & \cite{Zhang2020-wp}\\
        
        Domain specific measures & Comparison of the distributions according to a domain specific measure & Quantile-Quantile (Q-Q) plot (point-processes) & \cite{Xiao2017-lh}\\
        
        Classifier accuracy & Accuracy of a classifier trained to discriminate real from synthetic units. & - & \cite{Fisher2019,walsh2020generating}\\\bottomrule
        
    \end{tabular}
\end{table}

Statitistical fidelityA substitute to directly assessing the ability of the model to replicate the distribution of real data is to compare the information content or the real data against that of synthetic data. In other words, a statistical utility metric measures the value of the work that can be done with synthetic data. Primarily, authors attempt by various measures to determine if the statistical properties of the synthetic data distribution correspond to the the real distribution. These metrics are presented in Table ???. In general, statistical metrics do not offer convincing support for the quality of the synthetic data, they are often ambiguous or can be found to be misleading upon further investigation. Given the complexity of health data, low-dimensional transformations are unlikely to paint a full picture. Authors often state that no single metric taken on its own was sufficient, and that a combination of them allowed deeper understanding of the data. Synthetic data utility While utility-based metrics often provide a more convincing indicator of data realism, they mostly lack the interpretability that some statistical metrics allow. Methods aimed at evaluating the work that can be done with synthetic data are presented in Table 4. We divided these into two categories, those in which the task is of a more conceptual nature (Data utility metrics), and those based on tasks with real-world application (Application utility metrics). Note that this distinction is not based on a rigororous definition, but serves to facilitate understanding.

\begin{table}
    \caption{Metrics of data realism employing methods and measures based on evaluating the statistical properties of the synthetic data distribution, mostly in comparison with the distribution of real data\label{tab:statmetrics}} 
    
    \begin{tabular}{@{} p{0.2\textwidth} p{0.2\textwidth} p{0.2\textwidth} p{0.2\textwidth} @{}}\toprule
        Metric & Description & References\\ \midrule
        Dimensions-wise distribution (DWD) & A generative model is trained on the real data to generate a dataset of the same size. The Bernoillli success probability is compared between both datasets for each feature. & \cite{Beaulieu-Jones2019-ct,choi2017generating,chin2019generation,yan2020generating,Baowaly2019,Baowaly_2019,ozyigit2020generation}\\
        Interdimensional correlation & Dimenion-wise Pearson coefficient correlation matrices for both real and synthetic data are compared. & \cite{Beaulieu-Jones2019-ct, Goncalves2020}\cite{torfi2019generating,Frid_Adar_2018,Yang_2019,ozyigit2020generation}\\
        First-order proximity metric & {} & \cite{Zhang2020-wp}\\
        Log-cluster metric & {} & \cite{Goncalves2020}\\
        Support coverage metric & {} & \cite{Goncalves2020}\\
        Time-lagged correlations and covariates & {} & \cite{Fisher2019,walsh2020generating}\\
        Latent Space Representation (LSR) & {} & \cite{yan2020generating}\\
        Distribution of Jaccard similarity & {} & \cite{ozyigit2020generation}\\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}
        
        
        \caption{Metrics of data realism employing methods and measures based on evaluating the utility of the synthetic data on practical tasks.}\label{tab:aug-metrics}
        
        \begin{tabular}{@{} p{0.2\textwidth} p{0.2\textwidth} p{0.2\textwidth} @{}} \toprule
        Metric & Description & References\\ \midrule
        
        \multicolumn{3}{Y}{\textbf{Data utility metrics}}\\ \midrule
        
        Dimension-wise prediction (DWP) & Each variable is in turn chosen as the prediction target label and the remaining as features. Two predictors are trained to predict the label, one from the synthetic data and another from a portion of the real data. Their performance is compared on the left out real data.  & \cite{choi2017generating,Camino2018-re,Goncalves2020,yan2020generating}\\[20pt]
        
        Association Rule Mining (ARM) & & \cite{Baowaly2019,Bae2020,yan2020generating}\\[20pt]
        
        Discriminative Siamese architecture & & \cite{torfi2019generating}\\[20pt]
        
        Train on synthetic, test on real (TRTS) & Accuracy on real data of some form of predictor trained on synthetic data \cite{Beaulieu-Jones2019-ct}. Correlation between important features (RF) and model coefficients (LR and SVM) \cite{Beaulieu-Jones2019-ct}. & \cite{esteban2017real,Xu2019-ay,Yoon2018-dm,chin2019generation}\\
        
        Accuracy on synthetic data of some form of predictor trained on real data & & \cite{Bae2020}\\
        
        Forward prediction accuracy of conditional generative model &
        Models trained to make forward predictions from past observations or from real data transformed with a known function can simply be evaluated for accuracy. & \cite{Xiao2018-aj,mcdermott2018semi,yoon2018gain,Yang_2019b}\\
        

        \multicolumn{3}{Y}{\textbf{Applied utility metrics}}\\ \midrule

        
        Data augmentation & A predictor is trained on a combination dataset of real and synthetic data and performance is compared with the same predictor trained on real data alone. & \cite{Yoon2018-mo}\\
        
        Predictor augmentation & The trained generative model is incorporated into a predictor's activation function by generating an ensemble of proximate data points for each instance, thereby improving generalization. & \cite{Che_2017}\\
        
        \bottomrule
        
        \end{tabular}
\end{table}

\subsection{Alternative evaluation}
In their publications, Yale et al. propose refreshing approaches to evaluating the utility of synthetic data. For example, they organized a hack-a-thon type challenge. During the event, students were tasked with creating classifiers, while provided only with synthetic data \cite{Yale_2020}. They were then scored on the accuracy of their model in real data. Similarly, in a different evaluation experiment, they attempted (successfully) to recreate published medical papers based on the MIMIC dataset using only data generated from their model HealthGAN. The implications of these results for exploratory data analysis, reproducibility experiments in cases where data cannot be distributed and more generally education in health-related scientific training are glaring. In a subsequent paper, the authors evaluate the performance of their model against traditional privacy preservation methods by using the trained discriminator component of HealthGAN to d