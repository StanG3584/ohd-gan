\section{Results}
\subsection{Model validation and data evaluation}
To asses the solution to a generative modelling problem, it is necessary to validate the model obtained, and subsequently to verify its output. GANs aim to approximate a data distribution PP , using a parameterized model distribution QQ  (Borji 2018). Thus, in evaluating the model, the goal is to validate that the learning process has led to a sufficiently close approximation. Approaches to evaluation can be categorized as either quantitative or qualitative. 
Qualitative evaluation
The qualitative evaluation approaches found in the literature are mainly preference judgement, discrimination tasks, clinician evaluation (Borji 2018). Participants, such as medical professionals, discriminate between real and synthetic instances (Choi 2017b), or asked to rank the quality of real and synthetic samples on a numerical scale and their significance is determined with a Mann-Whitney U test (Beaulieu-Jones 2019). Similarly, visual inspection of statistics or projections of the data can help get a better understanding of model behaviour (Beaulieu-Jones 2019, Che 2017), but are often weak indicators of model performance without more objective  metrics (Jackson 2019). 
Quantitative evaluation
Comparing distributions
Numerous statistical metrics have been proposed or explored to compare the distributions of real and synthetic data (Borji 2018). We present here those employed in the publication included in the review in Tab. 2.

\input{35B-table-stats}
\input{35A-table-distribution}

Statitistical fidelityA substitute to directly assessing the ability of the model to replicate the distribution of real data is to compare the information content or the real data against that of synthetic data. In other words, a statistical utility metric measures the value of the work that can be done with synthetic data. Primarily, authors attempt by various measures to determine if the statistical properties of the synthetic data distribution correspond to the the real distribution. These metrics are presented in Table ???. In general, statistical metrics do not offer convincing support for the quality of the synthetic data, they are often ambiguous or can be found to be misleading upon further investigation. Given the complexity of health data, low-dimensional transformations are unlikely to paint a full picture. Authors often state that no single metric taken on its own was sufficient, and that a combination of them allowed deeper understanding of the data. Synthetic data utility While utility-based metrics often provide a more convincing indicator of data realism, they mostly lack the interpretability that some statistical metrics allow. Methods aimed at evaluating the work that can be done with synthetic data are presented in Table 4. We divided these into two categories, those in which the task is of a more conceptual nature (Data utility metrics), and those based on tasks with real-world application (Application utility metrics). Note that this distinction is not based on a rigororous definition, but serves to facilitate understanding.



%\input{35C-table-augmenation}

\subsection{Alternative evaluation}
In their publications, Yale et al. propose refreshing approaches to evaluating the utility of synthetic data. For example, they organized a hack-a-thon type challenge. During the event, students were tasked with creating classifiers, while provided only with synthetic data \cite{Yale_2020}. They were then scored on the accuracy of their model in real data. Similarly, in a different evaluation experiment, they attempted (successfully) to recreate published medical papers based on the MIMIC dataset using only data generated from their model HealthGAN. The implications of these results for exploratory data analysis, reproducibility experiments in cases where data cannot be distributed and more generally education in health-related scientific training are glaring. In a subsequent paper, the authors evaluate the performance of their model against traditional privacy preservation methods by using the trained discriminator component of HealthGAN to d