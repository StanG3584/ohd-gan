\section{Privacy Preservation}
To evaluate the risk of reidentification of synthetic data in the publications included, empirical analyses of privacy preservation are conducted according to the definitions of Membership Inference (MIA), Attribute Disclosure (AD)  \cite{choi2017generating,Goncalves2020,yan2020generating} and Reproduction rate \cite{Zhang2020-wp}. Cosine similarities between pairs of samples are also employed \cite{torfi2019generating}. All studies report low success rates for these types of attacks, while there is little effect from the sample size. Broadly, an MIA attack aims to determine if a particular record was used to train a machine learning model \cite{chen2019ganleaks}. There is no canonical process by which an attack is conducted, nor specification of the data assets initially in possession of the attacker. For a comprehensive taxonomy of MIA against GANs, refer to the suitably titled publication by Chen et al. in which medGAN was subjected to a number of trials.\par
In black-box and white-box type attacks, including the LOGAN \cite{hayes2017logan} method, medGAN performed considerably better than WGAN-GP \cite{gulrajani2017improved}, the algorithm which served as basis for improvements to medGAN in publications discussed in Section 3.1. Overall, the authors note that releasing the full model poses a high risk of privacy breaches and that smaller training sets (under 10k) also lead to a higher risk.\par  
AD is defined as the risk of an attacker correctly infering unknown attributes of a patient's record, given a number of known attributes. Goncalves et al. evaluated MC-medGAN against multiple non-adversarial generative models in a variety of privacy compromising attacks, including AD, obtaining inconsistent results for MC-medGAN \cite{Goncalves2020}. While this is not mentioned by the authors, multiple results reported in the publication point to the fact that the GAN was not properly trained or suffered mode-collapse.\par
Numerous attempts have been made to confer traditional privacy guarantees that deteriorate data, such as differentially-private stochastic gradient descent. By limiting the gradient amplitude at each step and adding random noise, AC-GAN could produce useful data witth $\epsilon=3.5$ and $\delta<10^{-5}$ according to the definition of differential privacy \cite{Beaulieu-Jones2019-ct, esteban2017real,chincheong2020generation}. \cite{BaeAnomiGAN2020}. Uniquely, Bae et al. ensure privacy with a probabilistic scheme that ensure indistinguishably, but also maximizes utility. Specifically, a multiplicative perturbation by random orthogonal matrices with input entries of $k Ã— m$ medical records and a second second discriminator in the form of a pretrained  predictor \cite{Bae2020}. Means to confer privacy guarantees on synthetic data generated by GANs are being actively researched in a variety of fields, many of which are a priori readily applicable to health data. At this stage, however, contradictory results have between obtained where the statistical fidelity of the synthetic seemed to be preserved, but utility-based measures based on a classification were degraded by incorporating DP.\par
\subsection{The status of fully synthetic in regards to current privacy regulations}
It seems intuitively possible that the artificial nature of synthetic data essentially prevents associations with real patients, however the question is never directly addressed in the publications. An extensive Stanford Technological Review legal analysis of synthetic data concluded that laws and regulations should not treat synthetic data indiscriminately from traditional privacy preservation methods \cite{bellovin2019privacy}. They state that current privacy statutes either outweigh or downplay the potential for synthetic data to leak secrets by implicitly including it as the equivalent of anynonymization. 
\subsection{GAN-centric approach to privacy}
Some have put forward the notion that preventing overfitting and preserving privacy may not be conflicting goals \cite{Wu2019-ui,Mukherjee2019-vu}. In privGAN, Mukherjee et al., an adversary is introduced, forcing the generator to produce samples that minimize the risk of MIA attack, in addition to cheating the discriminator. The combination of both goals has the explicit effect of preventing overfitting, and their algorithm produces samples of similar quality to non-private GANs.\par
The discordance between the theoretical concepts of DP, which are  based ultimately on infinite samples, and the often insufficient data on which the probability of disclosure is calculated remains deficient. Therefore, Yoon et al. have postulated an intriguing alternative view of privacy \cite{Yoon2020}. They propose to emphasize measuring identifiability of finite patient data, rather than the probabilistic disclosure loss of DP based on unrealistic premises. Simplistically, they define identifiability as the minimum closest distance between any pair of synthetic and real samples. In their implementation, the generator receives both the usual random seed and a real sample as input. This has the effect of mitigating mode collapse, but also of reproducing the real samples. On the other hand, the discriminator is equipped with an additional loss metric based on a measure of similarity between the original sample and the generated one, thus ensuring the tuneable threshold of identifiability is met. Their results on a number of previously discussed evaluation metrics are encouraging.\par
In a similar approach, Yale et al. broke away from the theoretical guarantees of traditional methods with a measure native to GANs. Their proposal is a metric quantifying the loss of privacy, a concept more aligned with the objective of GANs to minimize the loss of data utilit \cite{yale:hal-02160496,p2019}. They point out, quite appropriately, the advantage of concrete measureable values of loss in utility and privacy when making the decision of releasing sensitive data. Briefly, the Nearest Neighbor Adversarial Accuracy measures the loss in privacy based on the difference between two nearest neighbor metrics. The  first component is the proportion of synthetic samples that are closer to any real sample than any pair of real samples. The second component is the reverse operation. In a subsequent paper, HealthGAN evaluated against traditional privacy preservation methods with a variant of the IA based on the nearest neighbor metric. HealthGAN performs considerably better than all other methods, while still maintaining utility on a prediction task .

