\section{Results}
\subsection{Algorithms, models and training}
\subsubsection{Summary}
We have found a total of 36 publications describing the development or adaption of GAN algorithms for OHD, presented in Table \ref{tab:citeinc}. The type of data addressed in each of these publications can be generalized into one of two categories: time-dependent observations, such as time-series, or static representation in the form of feature tables. Publications giving consideration to privacy either perform privacy evaluations of their algorithms and synthetic data, or exclusively concentrate on comparing methods on the subject of privacy.

\begin{table}
  \centering
    \caption{Publications included}\label{tab:citeinc}
  
    \begin{tabular}{@{}lllll@{}} \toprule
    
    \multicolumn{1}{l}{Subtotal: 11} & \multicolumn{1}{l}{Subtotal: 5} & {} & \multicolumn{1}{l}{Subtotal: 12} & \multicolumn{1}{l}{Subtotal: 9}\\ \midrule
    
    \multicolumn{5}{l}{\textbf{Total publications: 40}}\\
    \bottomrule
    \end{tabular}
\end{table}

Most efforts are focused on adapting the current methods to the characteristics and complexities of OHD, of which multi-modality or non-Gaussian continuous features, heterogeneity, a combination of discrete and continuous features, longitudinal irregularity, correlation complexity, missingness or sparsity, class imbalance and noise are often cited. While these may pose a challenge for the development of suitable GAN methods, others properties make the prospect of success highly valuable. In fact, the most cited motivation to develop OHD-GAN is to cope with the often limited number of samples in medical datasets and to overcome the highly restricted access to OHD.\par

\section{Motivations for developing OHD-GAN}
The authors cite a wide range of potential applications for generative models of OHD. While some of these goals are optimistic and have yet to be realized, they paint an encouraging picture for the value OHD-GAN. We've listed a few recurring motivations here. 
\subsection{Data augmentation}
Data augmentation is mentioned in nearly all publications. Most commonly, synthetic data can improve generalization in predictive algorithms by providing additional information about the real data distribution \cite{Wang_2019,Che_2017,Yoon2018-dm, Yoon2018-mo}. Similarly, the application of GANs to domain translation and semi-supervised training approaches could support predictive tasks in healthcare that lack data with accurate labels, paired samples, or present class imbalance \cite{Che_2017,mcdermott2018semi}. 
\subsection{Enhancing privacy and increasing accessibility}
SD is seen as the key to unlocking the value of OHD, which is currently locked in due to privacy concerns. Preserving privacy can broadly be described as reducing the risk of re-identification attacks to an acceptable level. Many studies noted that highly restricted access to OHD is hindering machine learning, and more generally scientific progress \cite{Beaulieu-Jones2019-ct, Baowaly_2019,Che_2017,esteban2017real,Fisher2019}. Due to its artificial nature, SD is proposed as a means to forgo data use agreements, while potentially providing greater privacy guarantees and reducing the loss of utility \cite{Beaulieu-Jones2019-ct, Baowaly_2019,esteban2017real,Fisher2019,walsh2020generating}. Overall, enabling access to greater variety, quality and quantity of OHD could have positive effects in a wide range of fields, such as software development, education, and training of medical professionals. 
\subsection{Enabling precision medicine}
The ability to conduct simulations of disease progression for individual patients could have transformative impacts on healthcare. Generative models conditioned on a patient's baseline state could help inform clinical decision making by quantifying disease progression \cite{walsh2020generating, Fisher2019}. Additionally, stochastic simulations of individual patient profiles could help quantify risk at an unprecedented level of granularity \cite{Fisher2019}. Predicting patient-specific responses to drugs is still a new field of research, a problem known as Individualized Treatment Effects (ITE). The task of estimating ITEs is persistently hampered by the lack or paired samples, or counterfactuals \cite{Yoon2018-mo, chu2019treatment}. In regards to these issues, GANs have shown the ability for domain translation, mapping a sample from its to original class to the paired equivalent. This includes bidirectional transformations, in addition to the possibility of learning from very few paired samples, or even achieving better performance in the absence of paired samples \cite{Wolterink2017DeepMT}.
\subsection{From patient and disease models to digital twins}
Realistic synthetic data implies a model that approximates the process that generated the real information \cite{esteban2017real}. Achieving models of significant complexity would open up new simulation possibilities for developing predictive systems and methods. In clinical research, such models could help quantify cause and effect, simulate different study designs, provide control samples or more generally give us a better understanding of disease progression in relation to initial conditions \cite{Fisher2019, yahi2017generative, walsh2020generating}. Pushing the aspect of simulation further, the concept of "digital twins" represents in a way the ultimate realization of personalized medicine. A common practice in industrial sectors is high-fidelity virtual representations, or long-term simulations, of physical assets that grant a comprehensive understanding of the workings, behavior and life-cycle. Their state is continuously updated from theoretical data, real data, streaming IoT indicators and conditional synthetic data. In a position paper, Angulo et al. draw the parallels of this technique with the current needs in healthcare and the emergence of the necessary technologies for the proposal they bring forward \cite{angulo2019towards,Angulo_2020}. Notably, the rapid adoption of wearables that are continuously monitoring people's physiological state. Through continuous lifelong learning, patient models inform the decisions of medical professionals, but also enable testing research hypotheses. In their proposal, GANs are an essential component of the ecosystem to ensure patient privacy and to provide bootstrap data. Fisher et al. employ the term to describe their method \cite{walsh2020generating}.
\subsection{Data Types and Feature Engineering}
Few publications made use of OHD in its initial form. In most cases, feature engineering was used to adapt the data to the scientific question, or to make it intelligible for particular algorithms. The data is transformed into one of four modalities: time series, point-processes, ordered sequences or aggregates described in Fig. 2.

\begin{table}
  \caption{Types of observational health data and features engineering}\label{tab:typeseng}
  
  \begin{tabular}{llll} \toprule
  
  Type & Source and format & Challenges & Features engineering\\ \midrule
  
  Time-series & Automatic measurements at fixed time intervals by medical equipment or medical professionals. & Often sporadic, with many missing observations across time end dimensions. & Data imputation, imputation coupled with training, binning in into fixed-size intervals or combination of binning and imputation\\
  Point-processes & Time intervals between medical events, such as hospital visits. & & Timestamped events transformed into the time delta between each consecutive occurrences.\\
  Ordered sequences & Variable-length, ordered vectors of medical codes & Large number of codes and variable length. & Sequences are projected into a trained embedding that preserves semantic meaning
  according to methods borrowed from NLP.\\
  Tabular & Continuous, ordinal and categorical features in tabular form. & Mixture of discrete features with high class imbalance and multi-modal continuous features. & Medical history is aggregated into a fixed-size vector of binary or aggregated counts of occurrences and combined with demographic features.\\
  \bottomrule
  \end{tabular}
\end{table}

\subsection{Data oriented GAN development}
\subsubsection{Auto-encoders and categorical features}
To deal with the incompatibility of ordinal and categorical features with back-propagation, in the algorithm \thealgo{medGAN} Choi et al. pre-train an Autoencoder (AE) to project the samples to, and from, a continuous latent space representation \cite{choi2017generating}. The trained decoder portion of the AE then maps the latent-space representation of the generator back to discrete features. In a later effort, Jackson et al. used \algo{medGAN} on an extended dataset containing demographic and health system usage information with similar results to the original \cite{Jackson_2019}. Numerous efforts were made to improve on the performance of \algo{medGAN}. Among the first, Camino et al. built \thealgo{MC-medGAN} in which they modified the AE by adding a Gumbel-Softmax activation layer after splitting the output with a dense layer for each categorical variable and finally concatenating their output \cite{Camino2018-re}. The authors also adapted a GAN based on recent training techniques: Wassertein GAN (WGAN) \cite{arjovsky2017wasserstein} an WGAN with Gradient Penalty (WGAN-GP) \cite{gulrajani2017improved}. In brief, the Wasserstein distance is a measure of distance between two probability distributions used as the loss function that has the property of always providing a smooth gradient, generally avoiding mode collapse. \thealgo{MC-WGAN-GP} is built in the same manner as \algo{MC-medGAN} but with Softmax layers. The authors determined that the proposed alternatives gave better results in general, but that the choice of a model will depend on data characteristics, particularly sparsity. Wasserstein's distance was widely adopted by subsequent authors for its beneficial rapport with mode collapse and health data. Baoway et al. adapted medGAN based on WGAN-GP, and introduced a second adaptation from Boundary-seeking GAN (BGAN) \cite{hjelm2017boundaryseeking} which pushes the generator to produce samples that lie on the decision boundary of the discriminator, expanding the search space. Respectively termed \thealgo{MedWGAN} and \thealgo{MedBGAN}, the algorithms have led to improved data quality, particularly with \algo{MedBGAN} \cite{Baowaly_2019,Baowaly2019}. The \thealgo{HealthGAN} algorithm was also based on a combination of \algo{medGAN} and WGAN-GP, but includes a data transformation method adapted from the Synthetic Data Vault \cite{Patki_2016} to map categorical features to and from the unit numerical range \cite{Yale_2020}. 

\subsubsection{Forgoing the autoencoder}\label{noauto}

With EMR Wasserstein GAN (EMR-WGAN), Zhang et al. dispose of the AE component in medGAN and introduce a conditional training method, with along conditioned batch normalization and layer normalization techniques to stabilise training (Zhang 2020). The algorithm was further adapted by Yan et al. as Heterogeneous GAN (HGAN) to better account for the conditional distributions between multiple data types and enforce record-wise consistency. A recognized problem with medGAN was that it produced common-sense inconsistencies, such as gender mismatches in medical codes (Yan 2020, Choi 2017). In HGAN, constraints are enforced by adding specific penalities to the loss function, such as ranges for numerical categorical pairs and mutual exclusivity for pairs of binary features (Yan 2020). To develop Conditional Tabular GAN (CTGAN), Xu et al. presume that tabular data poses a challenge to GANs owing to the non-Gaussian multimodal distribution of continuous columns and imbalanced discrete columns. Their algorithm, composed of fully connected layers, was developed with adaptations to deal with both continuous and categorical features. For continuous features, it employs mode-specific normalization to capture the multiplicity of modes. For discrete features conditional training-by sampling is devised to resample discrete attributes evenly during training, while recovering the real distribution when generating data (Xu 2019). Other approaches include: corGAN, where the AE is questionably replaced by a 1-dimensional Convolutional AE (CAE) to capture neighboring feature correlations of the input vectors (Torfi 2019), and two basic feedforward networks based on Wassertein distance to evaluate the capacity of GANs to model heterogeneous data of dense and sparse medical features (Chin-Cheong 2019) and to reproduce statistical properties (Ozyigit 2020). Reproducing physiological time-series Esteban et al. used devise the Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) based on LSTM to generate a regular time-series of physiological measurements from bedside monitors (Esteban 2017). Curiously, the authors dismiss Wassertein's distance, stating that they did not find application in their experiments. In addition, each dimension of their time-series is generated independently from the others, where one would assume they are correlated. A considerable loss of accuracy is observed on their utility based metric.Task oriented GAN developmentSemi-supervised learning and conditional modelsTo develop ehrGAN, an algorithm for sequences of medical codes that has the ability to produce neighbouring records of an input patient, Che et al. combine an Encoder-Decoder Convolutional Neural Network (CNN) (Ranzato 2007) with Variational Contrastive Divergence (VCD) (Che 2017). The ehrGAN generator is trained to decode a random vector mixed with the latent space representation of a particular patient. In a semi-supervised learning approach, the trained ehrGAN model is then incorporated into the loss function of a predictor where it can help generalization by producing neighbors for each input sample. Semi-supervised learning approaches are commonly employed to augment the minority class in imbalanced datasets. The self-training and co-training methods use classifiers first trained on the portion of labelled data to predict the labels of unlabelled instances. The newly labelled samples with the highest confidence are added to the labelled set to retrain the classifiers. The process is repeated iteratively. Yang et al. improve on this type of approach by incorporating a GAN in the procedure (Yang 2019). The GAN is first trained on the labelled set and used to rebalance it. The standard iterative process involving the classifier ensemble is then executed until expansion ceases. As a final step, the GAN is trained on the expanded labelled set to generate an equal amount of augmentation data. The authors obtained improved performance in a number of classification tasks and multiple tablular datasets with their method.Correcting bias with domain translationTo address the heteogeneity of healthcare data from different sources, Yoon et al. combines the concepts of cycle-consistent domain translation from Cycle-GAN (Zhu 2017) and multi-domain translation from Star-GAN (Choi 2017a) to build RadialGAN to translate heterogeneous patient information from different hospitals, correcting features and distribution mismatches (Yoon 2018). An encoder-decoder pair per data endpoint is trained to map records to and from a shared latent representation. Individualized treatment effectsThe task of estimating Individualized Treatment Effects (ITE), the response of a patient to a certain treatment given a set of charaterizing features is an ongoing problem. This is due mainly to the fact that counterfactual outcomes are never observed or that treatment selection is highly biased (Yoon 2018a, McDermott 2018, Walsh 2020). In this regard, Yoon et al. employ a pair of GANs, named Generative Adversarial Nets for inference of Individualized Treatment Effects (GANITE), one for counterfactual imputation and another for ITE estimation (Yoon 2018a). The former captures the uncertainty in unobserved outcomes by generating a variety of conterfactuals. The output is fed to the latter, which estimates treatment effects and provides confidence intervals. With Cycle Wasserstein Regression GAN (CWR-GAN), a joint regression-adversarial model, McDermott et al. demonstrated a semi-supervised approach also inspired by Cycle-GAN to leverage large amounts of unpaired pre/post-treatment time-series in ICU data for the estimation of ITE on physiological time-series (McDermott 2018). The algorithm has the ability to learn from unpaired samples, with very few paired samples, to reversibly translate the pre and post-treatment physiological series. Chu et al. approach the problem of data scarcity for ITEs by designing ADTEP, an algorithm that can maximize use of the large volume of EHR data formed by triples of non-task specific patient features, treatment interventions and treatment outcomes (Chu 2019). The ADTEP algorithm they developed learns representation and discriminatory features of the patient, and treatment data by training an AE for each pair of features. In addition to AE reconstruction loss, a second model is tasked with identifying fake treatment feature reconstructions. Finally, a fourth loss metric is calculated by feeding the concatenated latent representations of both AE to a logisitic regression model aimed at predicting the treatment outcome (Chu 2019). In the form of an ITE task, Wang et al. demonstrated an interesting algorithm to generate a time series of patient states and medication dosages using LSTM. In contrast to RGAN and RCGAN, in Sequentially Coupled Generative Adversarial Network (SC-GAN), patients state at the current timestep informs the concurrent medication dosage, which in turn affects the patient state in the upcoming timestep (Wang 2019). SC-GAN overcame a number of baselines on both statistical and utility metrics. Data Imputation with GANsGANs are naturally suited for data imputation, and could provide a new approach to deal with the problems of health data relating to sparsity. Statistical models developed for the multiple imputation problem increase quadraticly in complexity with the number of features, while the expressiveness of deep neural networks can model all features with missing values simultaneously efficiently. In that regard, Yoon et al. adapted the standard GAN to perform imputations on continuous features missing at random in tabular datasets (Yoon 2018b). In their algorithm GAIN, the discriminator is tasked with classifying individual variables as real or fake (imputed), as opposed to the whole ensemble. Additional input, or hint, containing the probability of each component being real or imputed is fed to the discriminator to resolve the multiplicity of optimal distributions that the generator could reproduce. The model performs considerably better than five state-of-the-art benchmarks. The GAIN algorithm was later adapted to also handle categorical features using fuzzy binary encoding, the same technique employed in HealthGAN (Yale 2019)Data augmentationThe distribution estimated by a generator model can compensate for lack of diversity in a real sample, essentially filling in the blanks in a manner comparable to data imputation. In such cases, data sampled from this distribution has the potential to help improve generalization in training predictive models. We find evidence of this by way of generating unobserved counterfactual outcomes (Yoon 2018a), or generating neighboring samples to help generalization in predictors (Che 2017). The RBM developed by Fisher et al. enabled them to simulate individualized patient trajectories based on their base state characteristics. Due to the stochastic nature of the algorithm, generating a large number of trajectories for a single patient can provide new insights of the influence of starting conditions on disease progression or quantify risk (Fisher 2019).

Results: Model validation and data evaluation
To asses the solution to a generative modelling problem, it is necessary to validate the model obtained, and subsequently to verify its output. GANs aim to approximate a data distribution PP​, using a parameterized model distribution QQ​ (Borji 2018). Thus, in evaluating the model, the goal is to validate that the learning process has led to a sufficiently close approximation. Approaches to evaluation can be categorized as either quantitative or qualitative. 
Qualitative evaluation
The qualitative evaluation approaches found in the literature are mainly preference judgement, discrimination tasks, clinician evaluation (Borji 2018). Participants, such as medical professionals, discriminate between real and synthetic instances (Choi 2017b), or asked to rank the quality of real and synthetic samples on a numerical scale and their significance is determined with a Mann-Whitney U test (Beaulieu-Jones 2019). Similarly, visual inspection of statistics or projections of the data can help get a better understanding of model behaviour (Beaulieu-Jones 2019, Che 2017), but are often weak indicators of model performance without more objective  metrics (Jackson 2019). 
Quantitative evaluation
Comparing distributions
Numerous statistical metrics have been proposed or explored to compare the distributions of real and synthetic data (Borji 2018). We present here those employed in the publication included in the review in Tab. 2.

\begin{table}
    \caption{Metrics employed to validate trained models based on the comparison of distributions.\label{tab:evaldist}} 
        
    \begin{tabular}{@{} p{0.2\textwidth} p{0.2\textwidth} p{0.2\textwidth} p{0.2\textwidth} @{}}\toprule
        
        Metric & Description & Example & References\\\midrule
        
        Kullback-Leibler (KL) divergence & Compares the distributions isolated features by measuring the similarity of their marginal probability mass functions (PMF). & - &
        \cite{Goncalves2020}\\
        
        Maximum Mean Discrepancy (MMD) & 
        Checks the dissimilarity between the real and synthetic probability distributions using samples drawn independently from each other. & - &
        \cite{esteban2017real}\\
        
        2-sample test (2-ST) & Answers whether two samples, the real and synthetic, originate from the same distribution through the use of a statistical test. & 
        Kolmogorov-Smirnov (KS) & 
        \cite{Fisher2019,Baowaly2019}\\
        
        Distribution of Reconstruction Error & 
        Determine if the samples in the synthetic set are more similar to those in the training set than those in the testing set. & Nearest-neighbor &
        \cite{esteban2017real}\\
        
        Latent projection distribution & 
        Compares the distribution of real and synthetic samples projected back into the latent space & Mean of the variance & \cite{Zhang2020-wp}\\
        
        Domain specific measures & Comparison of the distributions according to a domain specific measure & Quantile-Quantile (Q-Q) plot (point-processes) & \cite{Xiao2017-lh}\\
        
        Classifier accuracy & Accuracy of a classifier trained to discriminate real from synthetic units. & - & \cite{Fisher2019,walsh2020generating}\\\bottomrule
        
    \end{tabular}
\end{table}

Statitistical fidelityA substitute to directly assessing the ability of the model to replicate the distribution of real data is to compare the information content or the real data against that of synthetic data. In other words, a statistical utility metric measures the value of the work that can be done with synthetic data. Primarily, authors attempt by various measures to determine if the statistical properties of the synthetic data distribution correspond to the the real distribution. These metrics are presented in Table ???. In general, statistical metrics do not offer convincing support for the quality of the synthetic data, they are often ambiguous or can be found to be misleading upon further investigation. Given the complexity of health data, low-dimensional transformations are unlikely to paint a full picture. Authors often state that no single metric taken on its own was sufficient, and that a combination of them allowed deeper understanding of the data. Synthetic data utility While utility-based metrics often provide a more convincing indicator of data realism, they mostly lack the interpretability that some statistical metrics allow. Methods aimed at evaluating the work that can be done with synthetic data are presented in Table 4. We divided these into two categories, those in which the task is of a more conceptual nature (Data utility metrics), and those based on tasks with real-world application (Application utility metrics). Note that this distinction is not based on a rigororous definition, but serves to facilitate understanding.

\begin{table}
    \caption{Metrics of data realism employing methods and measures based on evaluating the statistical properties of the synthetic data distribution, mostly in comparison with the distribution of real data\label{tab:statmetrics}} 
    
    \begin{tabular}{@{} p{0.2\textwidth} p{0.2\textwidth} p{0.2\textwidth} p{0.2\textwidth} @{}}\toprule
        Metric & Description & References\\ \midrule
        Dimensions-wise distribution (DWD) & A generative model is trained on the real data to generate a dataset of the same size. The Bernoillli success probability is compared between both datasets for each feature. & \cite{Beaulieu-Jones2019-ct,choi2017generating,chin2019generation,yan2020generating,Baowaly2019,Baowaly_2019,ozyigit2020generation}\\
        Interdimensional correlation & Dimenion-wise Pearson coefficient correlation matrices for both real and synthetic data are compared. & \cite{Beaulieu-Jones2019-ct, Goncalves2020}\cite{torfi2019generating,Frid_Adar_2018,Yang_2019,ozyigit2020generation}\\
        First-order proximity metric & {} & \cite{Zhang2020-wp}\\
        Log-cluster metric & {} & \cite{Goncalves2020}\\
        Support coverage metric & {} & \cite{Goncalves2020}\\
        Time-lagged correlations and covariates & {} & \cite{Fisher2019,walsh2020generating}\\
        Latent Space Representation (LSR) & {} & \cite{yan2020generating}\\
        Distribution of Jaccard similarity & {} & \cite{ozyigit2020generation}\\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}
        
        
        \caption{Metrics of data realism employing methods and measures based on evaluating the utility of the synthetic data on practical tasks.}\label{tab:aug-metrics}
        
        \begin{tabular}{@{} p{0.2\textwidth} p{0.2\textwidth} p{0.2\textwidth} @{}} \toprule
        Metric & Description & References\\ \midrule
        
        \multicolumn{3}{Y}{\textbf{Data utility metrics}}\\ \midrule
        
        Dimension-wise prediction (DWP) & Each variable is in turn chosen as the prediction target label and the remaining as features. Two predictors are trained to predict the label, one from the synthetic data and another from a portion of the real data. Their performance is compared on the left out real data.  & \cite{choi2017generating,Camino2018-re,Goncalves2020,yan2020generating}\\[20pt]
        
        Association Rule Mining (ARM) & & \cite{Baowaly2019,Bae2020,yan2020generating}\\[20pt]
        
        Discriminative Siamese architecture & & \cite{torfi2019generating}\\[20pt]
        
        Train on synthetic, test on real (TRTS) & Accuracy on real data of some form of predictor trained on synthetic data \cite{Beaulieu-Jones2019-ct}. Correlation between important features (RF) and model coefficients (LR and SVM) \cite{Beaulieu-Jones2019-ct}. & \cite{esteban2017real,Xu2019-ay,Yoon2018-dm,chin2019generation}\\
        
        Accuracy on synthetic data of some form of predictor trained on real data & & \cite{Bae2020}\\
        
        Forward prediction accuracy of conditional generative model &
        Models trained to make forward predictions from past observations or from real data transformed with a known function can simply be evaluated for accuracy. & \cite{Xiao2018-aj,mcdermott2018semi,yoon2018gain,Yang_2019b}\\
        

        \multicolumn{3}{Y}{\textbf{Applied utility metrics}}\\ \midrule

        
        Data augmentation & A predictor is trained on a combination dataset of real and synthetic data and performance is compared with the same predictor trained on real data alone. & \cite{Yoon2018-mo}\\
        
        Predictor augmentation & The trained generative model is incorporated into a predictor's activation function by generating an ensemble of proximate data points for each instance, thereby improving generalization. & \cite{Che_2017}\\
        
        \bottomrule
        
        \end{tabular}
\end{table}

\subsection{Alternative evaluation}
In their publications, Yale et al. propose refreshing approaches to evaluating the utility of synthetic data. For example, they organized a hack-a-thon type challenge. During the event, students were tasked with creating classifiers, while provided only with synthetic data \cite{Yale_2020}. They were then scored on the accuracy of their model in real data. Similarly, in a different evaluation experiment, they attempted (successfully) to recreate published medical papers based on the MIMIC dataset using only data generated from their model HealthGAN. The implications of these results for exploratory data analysis, reproducibility experiments in cases where data cannot be distributed and more generally education in health-related scientific training are glaring. In a subsequent paper, the authors evaluate the performance of their model against traditional privacy preservation methods by using the trained discriminator component of HealthGAN to d