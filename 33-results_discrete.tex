\subsection{Data oriented GAN development}
\subsubsection{Auto-encoders and categorical features}
In what is, to the best of our knowledge the first attempt at developing a GAN for OHD. Choi et al. focus on the problem posed by the incompatibility of categorical and ordinal features with back-propagation. Their solution is to pretrain an Autoencoder (AE) to project the samples to and from a continuous latent space representation and retain the decoder portion to form a component of the GAN \cite{Choi2017-nt}. In the algorithm \thealgo{medGAN,} the trained decoder in incorporated into the generator and maps the randomly sampled input vectors from latent space representation back to discrete features. This first exemplar of synthetic OHD generated by GAN inspires a series of enhancements.\par

Numerous efforts were made to improve the performance of \algo{medGAN}. Among the first, Camino et al. developed \thealgo{MC-medGAN} in which they modified the AE by adding a Gumbel-Softmax \cite{jang2016categorical} activation layer after splitting its output with a dense layer for each categorical variable and finally concatenating of the Gumbel-Softmax \todo layers \cite{Camino2018-re}. The authors also developed an adaptation based on recent training techniques: Wassertein GAN (WGAN) \cite{arjovsky2017wasserstein} and a WGAN with Gradient Penalty (WGAN-GP) \cite{gulrajani2017improved}. In brief, the Wasserstein distance is a measure of distance between two probability distributions that has the property of always providing a smooth gradient. When used as the loss function of the discriminator, it generally improves training stability and mitigates mode collapse. Weight clipping is used in WGAN to ensure the discriminator lies within of 1-Lipschitz functions. The undesirable effects of weight clipping are eliminated by rather imposing a penalty on the gradient . \thealgo{MC-WGAN-GP} is the equivalent of \algo{MC-medGAN} but with Softmax layers. The authors report that the choice of a model will depend on data characteristics, particularly sparsity.\par 
Wasserstein's distance was widely adopted by subsequent authors for its compatibility with OHD. Baowaly et al. developed \thealgo{MedWGAN} also based on WGAN, and \thealgo{MedBGAN} borrowing from Boundary-seeking GAN (BGAN) \cite{hjelm2017boundaryseeking} which pushes the generator to produce samples that lie on the decision boundary of the discriminator, expanding the search space. Both led to improved data quality, in particular \algo{MedBGAN} \cite{baowaly_2019_IEEE,baowaly_2019_jamia}. In other effort, Jackson et al. tested \algo{medGAN} on an extended dataset containing demographic and health system usage information, obtaining results similar to those of the original \cite{Jackson_2019}. The \thealgo{HealthGAN} built upon WGAN-GP, but includes a data transformation method adapted from the Synthetic Data Vault \cite{Patki_2016} to map categorical features to and from the unit numerical range \cite{Yale_2020}. 