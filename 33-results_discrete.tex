\subsection{Data oriented GAN development}\label{subsec:data_gan_dev}

\subsubsection{Auto-encoders and categorical features}\label{subsubsec:categorical}
In what is to the best of our knowledge, the first attempt at developing a GAN for OHD. Choi et al. focus on the problem posed by the incompatibility of categorical and ordinal features with back-propagation \todo{quick cite}. Their solution is to pretrain an \gls{ae} to project the samples to and from a continuous latent space representation \todo{what is the advantage is this representation}. The decoder portion is retained along with its trained weights to form a component of medGAN \cite{choi2017generating}. It is incorporated into the generator and maps the randomly sampled input vectors from latent space representation back to discrete features. This first exemplar of synthetic OHD generated by GAN inspires a series of enhancements.\par

Numerous efforts were made to improve the performance of \algo{medGAN}. Among the first, Camino et al. developed \thealgo{MC-medGAN} in which they modified the \gls{ae} component by splitting its output into a Gumbel-Softmax \cite{jang2016categorical} activation layer for each categorical variable and concatenating the results. \cite{Camino2018-re}. The authors also developed an adaptation based on recent training techniques: Wassertein GAN (WGAN) \cite{arjovsky2017wasserstein} and a WGAN with Gradient Penalty (WGAN-GP) \cite{gulrajani2017improved}. In brief, the Wasserstein distance is a measure between two probability distributions that has the property of always providing a smooth gradient. When used as the loss function of the discriminator, it generally improves training stability and mitigates mode collapse. The Wassertein loss function a 1-Lipschitz constraint that was originally solved by weight clipping. It was however demonstrated that in some cases this prevented the network from modelling the optimal function, thus Gradient penalty, a less restrictive regularization was introduced \cite{Petzka2018}. \thealgo{MC-WGAN-GP} is the equivalent of \algo{MC-medGAN} but with Softmax layers. The authors report that the choice of a model will depend on data characteristics, particularly sparsity.\par 
Wasserstein's distance was widely adopted by subsequent authors owing to the propensity of OHD to induce mode collapse. Baowaly et al. developed \thealgo{MedWGAN} also based on WGAN, and \thealgo{MedBGAN} borrowing from Boundary-seeking GAN (BGAN) \cite{hjelm2017boundaryseeking} which pushes the generator to produce samples that lie on the decision boundary of the discriminator, expanding the search space. Both led to improved data quality, in particular \algo{MedBGAN} \cite{baowaly_2019_IEEE,baowaly_2019_jamia}. In other effort, Jackson et al. tested \algo{medGAN} on an extended dataset containing demographic and health system usage information, obtaining results similar to the original \cite{Jackson_2019}. The \thealgo{HealthGAN} built upon WGAN-GP, but includes a data transformation method adapted from the Synthetic Data Vault \cite{Patki_2016} to map categorical features to and from the unit numerical range \cite{Yale_2020}. 