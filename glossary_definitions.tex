
%%%%%%%%%%%%%%%%%%%%%%%%%%% Algorithms
\newglossaryentry{gumbel-gan}
{
        name=Gumbel-Softmax GAN,
        description={}
}

\newglossaryentry{arae}
{
        name=ARAE,
        description={Adversarially regularized autoencoders.}
}

\newglossaryentry{t-sne}{
    name=t-SNE,
    description={The t-Distributed Stochastic Neighbor Embedding clustering algorithm is a nonlinear dimensionality reduction technique commonly applied to high-dimensiona data. See \citet{maaten2008tsne}.}
}

\newglossaryentry{mode-collapse}
{
        name=mode collapse,
        description={The training procedure fails to converge, or converges to an undesirable local minima resulting in a lack of variety in the generated samples.}
}

\newglossaryentry{feed-forward}
{
        name=feed-forward network,
        description={Basic Neural Network in its simplest form.}
}

\newglossaryentry{mb-avg}
{
        name=Mini-batch averaging,
        description={Adaptation of mini-batch averaging to cope with mode collapse, see \cite{choi2017generating}}
}

\newglossaryentry{dom-tran}{
    name=domain translation,
    description={Transforming data points from one domain or category to another.}
}

\newglossaryentry{semi-sup}{name=semi-supervised, description={\todo{definition}
}}

\newglossaryentry{re-iden}{name=reidentification attack, description={\todo{definition}
}}

\newglossaryentry{dbio}{name=digital bio-markers, description={\todo{definition}
}}

\newglossaryentry{exploding}{name=exploding gradient,
    description={The gradients accumulate large amounts of error, destabilising or disabling the training procedure.}
}

\newglossaryentry{vanishing}{name=vanishing gradient, description={The gradients become null and the network can no longer be updated.
}}

\newglossaryentry{a-disclosure}{name=Attribute disclosure, description={\todo{definition}}}

\newglossaryentry{mem-inference}{name=Membership inference, description={\todo{definition}}}

\newglossaryentry{repro-rate}{name=Reproduction rate, description={\todo{definition}}}

\newglossaryentry{utility-metric}{name=utility-based metric, description={In a broad sense, any metric that measure the amount of work that can be done with the data}}

\newglossaryentry{iteff}{name=Individual Treatment Effects, description={\todo{definition}
}}
\newglossaryentry{pmed}{name=Personalized Medicine, description={\todo{definition}
}}

\newglossaryentry{}{name=, description={
}}

%% Missingness
\newglossaryentry{mar}{name=Missing at Random, description={Given a dataset with missing entries , the missingness depends only on the observed variables \cite{yoon2018imputation}.
}}

\newglossaryentry{mcar}{name=Missing Completely at Random, description={Given a dataset with missing entries, the missingness is not dependant on any of the variables, thus occurs completely at random \cite{yoon2018imputation}.
}}

%% Self and co-training semi-supervised training
\newglossaryentry{co-training}{name=co-training, description={The self-training and co-training methods use classifiers first trained on the portion of labelled data to predict the labels of unlabelled instances. The newly labelled samples with the highest confidence are added to the labelled set to retrain the classifiers. The process is repeated iteratively. In the words of \cite{yu2019rare}, \textit{"[...] co-training splits the features of labeled set into two sub-sets as two views, which are conditionally independent. Two classifiers are trained on two sub-sets respectively, and classify the unlabeled set with pseudo labeled. Then, the most confident unlabeled data determined by one classifier is fed into another classifier as additional pseudo labeled data for further training." \cite{yu2019rare}.}
}}

\newglossaryentry{self-training}{name=self-training, description={The self-training and co-training methods use classifiers first trained on the portion of labelled data to predict the labels of unlabelled instances. The newly labelled samples with the highest confidence are added to the labelled set to retrain the classifiers. The process is repeated iteratively. In the words of \citeauthor{yu2019rare}, \textit{"[...] a classifier is initially trained on the small labeled set, and the trained classifier is used to classify the unlabeled set, which is assigned with pseudo labels. After that, the part of unlabeled set with the most confident pseudo labels are selected, and added into the labeled set. The classifier iteratively trains itself with the labeled data and selected unlabeled data."} \cite{yu2019rare}.}
}

%% Privacy
\newglossaryentry{mia}{name=Membership Inference Attack, description={Broadly, an MIA attack aims to determine if a particular record was used to train a machine learning model \cite{chen2019ganleaks}. There is no canonical process by which an attack is conducted, nor specification of the data assets initially in possession of the attacker. For a comprehensive taxonomy of MIA against \gls{gan}, refer to the suitably titled publication by \citeauthor{chen2019ganleaks} in which \gls{medgan} was subjected to a number of trials.}}

%% Training techniques
\newglossaryentry{msn}{
 type=\acronymtype,
 name={MSN},
 description={Per feature, a variational Gaussian mixture model is used to estimate the number of modes and fit a Gaussian mixture. A one-hot vector indicating the mode, and a scalar indicating the value within the mode is produced. See \cite{Xu2019-ay}.},
 text={MSN},
 first={Mode-specific normalization (MSN)}
}

\newglossaryentry{tbs}{
    type=\acronymtype,
    name={TbS}, 
    description={To deal with the imbalance of values in categorical featues, during training the data is resampled in a way that all the categories from discrete attributes are sampled evenly, without inducing bias and so as to recover real data distribution. See \cite{Xu2019-ay} for a step-by-step spefication.}
    text={TbS},
    first={Training by sampling (TbS)}  
}

\newglossaryentry{nnaa}{
    type=\acronymtype,
    name={NN-AA}, 
    description={"Compares the distance from one point in a target distribution T, to the nearest point in a source distribution S, to the distance to the next nearest point in the target distribution." See \cite{yale2019ESANN}.}
    text={NN-AA},
    first={Nearest-neighbor Adversarial Accuracy (NN-AA)}
}

\newglossaryentry{pl}{
    type=\acronymtype,
    name={PL}, 
    description={Difference of \gls{NN-AA} on the test set and on the training set. See \cite{yale2019ESANN}.}
    text={PL},
    first={Privacy loss (PL)}
}

\newglossaryentry{dt}{
    type=\acronymtype,
    name={DT}, 
    description={The discriminator is tested on batches of synthetic data produced by other methods to asses the possibility of overfitting, see \cite{yale2019ESANN}.}
    text={DT},
    first={Discriminator testing (DT)}
}

\newglossaryentry{do}{
    type=\acronymtype,
    name={DO}, 
    description={Privacy preservation method. See \cite{yale2019ESANN} based on \cite{Dwork2008, Prasser2017}.}
    text={DO},
    first={Data obfuscation (DO}
}

\newglossaryentry{pate}{
    type=\acronymtype,
    name={PATE}, 
    description={Differntial privacy method: "The approach combines, in a black-box fashion, multiple models trained with disjoint datasets, such as records from different subsets of users. Because they rely directly on sensitive data, these models are not published, but instead used as "teachers" for a "student" model. The student learns to predict an output chosen by noisy voting among all of the teachers, and cannot directly access an individual teacher or the underlying data or parameters. The student's privacy properties can be understood both intuitively (since no single teacher and thus no single dataset dictates the student's training) and formally, in terms of differential privacy." \cite{Papernot2017,Papernot2018}}
    text={PATE},
    first={Private Aggregation of Teacher Ensembles (PATE)}
}

\newglossaryentry{mbd}{
    type=\acronymtype,
    name={MBD}, 
    description={Training technique. See \cite{Salimans2016}}
    text={MBD},
    first={Mini-batch discrimination (MDB)}
}

\newglossaryentry{t-gan}{
    type=\acronymtype,
    name={T-GAN}, 
    description={Training technique to stabilise training. Allows the introduction of real sample information into the process of training the the generator. See \cite{Jolicoeur-Martineau2019, Su2018}}
    text={T-GAN},
    first={Turing \gls{gan}}
}

\newglossaryentry{corrnn}{
    type=\acronymtype,
    name={CorrNN}, 
    description={Learns a common representation of two views, taking into account their correlation. See \cite{Jolicoeur-Martineau2019, Su2018}}
    text={CorrNN},
    first={Correlation \gls{nn}}
}

\newglossaryentry{fullbba}{
    type=\acronymtype,
    name={FullBB}, 
    description={A \gls{mi} attack setting where an attacker has now knowledge of the internal workings of the generator, but can only sample from it.}
    text={FullBB},
    first={Full Black-box Attack}
}

\newglossaryentry{partbba}{
    type=\acronymtype,
    name={PartBB}, 
    description={Similar to to the \gls{fullBB} setting with the attacker havign the additional knowoledge about the latent input $z$.}
    text={PartBB},
    first={Partial Black-box Attack}
}

\newglossaryentry{wba}{
    type=\acronymtype,
    name={WBA},
    description={Similar to the \gls{partbba} and \gls{fullbba} settings, but with the attacker having full knowledge of the generator internals, including gradient information.},
    text={WBA},
    first={White-box Attack}
}

\newglossaryentry{bgan}{
    type=\acronymtype,
    name={BGAN},
    description={Method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, enabling back-propagation. Tends to push the generated samples to lie on the decision boundary of the discriminator, which also improves stability of training on continuous data \cite{hjelm2017boundaryseeking}},
    text={BGAN},
    first={Boundary-seeking \gls{gan}}
}

\newglossaryentry{sdv}{
    type=\acronymtype,
    name={SDV},
    description={Generative model for relational database based on Gaussian Copulas \cite{Patki_2016}. One of the few publications treating multi-relation tables in their original form (to out knowledge the only), and has attracted a fair readership. See \href{https://github.com/sdv-dev/SDV}{Github sdv-dev/SDV}.},
    text={SDV},
    first={Synthetic Data Vault}
}


%\newacronym{}{Grouped CorrNN}{Grouped Correlation Neural Network}

% \newglossaryentry{⟨label ⟩}{type=\acronymtype,
% name={⟨abbrv ⟩},
% description={⟨long⟩},
% text={⟨abbrv ⟩},
% first={⟨long⟩ (⟨abbrv ⟩)},
% plural={⟨abbrv ⟩\glspluralsuffix},
% firstplural={⟨long⟩\glspluralsuffix\space (⟨abbrv ⟩\glspluralsuffix)},
% ⟨key-val list⟩}

%% Algorithms
\newacronym{ml}{ML}{Machine Learning}
\newacronym{nn}{NN}{Neural Network}
\newacronym{gan}{GAN}{Generative Adversarial Network}
\newacronym{ohd-gan}{OHD-GAN}{\glspl{gan} for Observation Health Data}
\newacronym{ffn}{FFN}{\gls{feed-forward} Network}
\newacronym{ae}{AE}{Autoencoder}
\newacronym{rnn}{RNN}{Recurrent \gls{nn}}
\newacronym{lstm}{LSTM}{Long Short-term Memory}
\newacronym{cgan}{CGAN}{Conditional \gls{gan}}
\newacronym{crmb}{CRMB}{Conditional Restricted Boltzmann Machine}
\newacronym{cnn}{CNN}{Convolutional \gls{nn}}
\newacronym{wgan}{WGAN}{Wassertein \gls{gan}}
\newacronym{beta-vae}{\ensuremath{\beta}-VAE}{\ensuremath{\beta} variational auto-encoder}
\newacronym{lr}{LR}{Logistic-regression}
\newacronym{cycle-gan}{Cycle-GAN}{Cycle-consistent \gls{gan}}
\newacronym{adtep}{ADTEP}{Adversarial Deep Treatment Effect Prediction}
\newacronym{cae}{CAE}{Convolutional \gls{AE}}

\newacronym[type=oalgo]{medgan}{medGAN}{medGAN}
\newacronym[type=oalgo]{ssl-gan}{SSL-GAN}{Semi-supervised Learning with a learned ehrGAN}
\newacronym[type=oalgo]{wgantpp}{WGANTPP}{\gls{wgan} for Temporal Point-processes}
\newacronym[type=oalgo]{radialgan}{RadialGAN}{RadialGAN}
\newacronym[type=oalgo]{mc-arae}{MC-ARAE}{Multi-categorical gls{arae}}
\newacronym[type=oalgo]{ctgan}{CTGAN}{Conditional Tabular \Gls{gan}}
\newacronym[type=oalgo]{heterogan}{HGAN}{Heterogeneous GAN}
\newacronym[type=oalgo]{emr-wgan}{EMR-WGAN}{EMR Wassertein GAN}
\newacronym[type=oalgo]{corgan}{corGAN}{corGAN}
\newacronym[type=oalgo]{1d-cae}{1D-CAE}{1-dimensional Convolutional \gls{ae}}
\newacronym[type=oalgo]{ehrgan}{ehrGAN}{Electronic Health Record GAN}
\newacronym[type=oalgo]{rgan}{RGAN}{Recurrent \gls{gan}}
\newacronym[type=oalgo]{rcgan}{RC-GAN}{Recurrent Convolutional \gls{gan}}
\newacronym[type=oalgo]{ganite}{GANITE}{Generative Adversarial Nets for inference of Individualized Treatment Effects}
\newacronym[type=oalgo]{cwr-gan}{CWR-GAN}{Cycle Wasserstein Regression \gls{gan}}
\newacronym[type=oalgo]{gain}{GAIN}{Generative Adversarial Imputation Network}
\newacronym[type=oalgo]{cgain}{CGAIN}{Categorical \gls{gain}}
\newacronym[type=oalgo]{mc-medgan}{MC-medGAN}{Multi-categorical \gls{medgan}}
\newacronym[type=oalgo]{mc-gumbelgan}{MC-GumbelGAN}{Multi-categorical Gumbel-softmax \gls{gan}}
\newacronym[type=oalgo]{mc-wgan-gp}{MC-WGAN-GP}{Multi-categorical \gls{wgan} with Gradient Penalty}
\newacronym[type=oalgo]{medbgan}{MedBGAN}{Boundary-seeking \gls{medgan}}
\newacronym[type=oalgo]{healthgan}{HealthGAN}{}
\newacronym[type=oalgo]{medwgan}{MedWGAN}{Wassertein \gls{medgan}}
\newacronym[type=oalgo]{sc-gan}{SC-GAN}{Sequentially Coupled \gls{gan}}
\newacronym[type=oalgo]{rmb}{RMB}{Restricted Boltzmann Machine}
\newacronym[type=oalgo]{anomigan}{AnomiGAN}{GANs for anonymizing private medical data}
\newacronym[type=oalgo]{wgan-gp}{WGAN-GP}{\gls{wgan} with Gradient Penalty}
\newacronym[type=oalgo]{dp-auto-gan}{DP-auto-GAN}{\gls{DP}-auto-\gls{gan}}
\newacronym[type=oalgo]{ads-gan}{ADS-GAN}{Anonymization through data synthesis using \gls{gan}}
\newacronym[type=oalgo]{gcgan}{GcGAN}{\gls{corrnn} and \gls{t-wgan}}
\newacronym[type=oalgo]{t-wgan}{T-wGAN}{Wassertein \gls{t-gan}}
\newacronym[type=oalgo]{conan}{CONAN}{\textit{Co}plementary patter\textbf{n A}augmentatio\textbf{n}}
\newacronym[type=oalgo]{cwgan-gp}{cWGAN-GP}{Conditional \gls{wgan-gp}}
\newacronym[type=oalgo]{pate-gan}{PATE-GAN}{Private Aggregation of Teacher Ensembles (PATE) framework applied to GANs}
\newacronym[type=oalgo]{ac-gan}{AC-GAN}{Auxiliary Classifier \gls{gan}}
\newacronym[typr=oalgo]{glugan}{GluGAN}{Blood Glucose \gls{gan}}
%
%\newacronym{}{}{}

%% Terms
\newacronym{sd}{SD}{Synthetic Data}
\newacronym{ohd}{OHD}{Observational Health Data}
\newacronym{ehr}{EHR}{Electronic Health Record}
\newacronym{icu}{ICU}{Intensive Care Unit}
\newacronym{pmf}{PMF}{Probability Mass Function}
\newacronym{ssl}{SSL}{Semi-supervised learning}
\newacronym{cqm}{CQM}{Clinical Quality Measure}
%% Fields
\newacronym[seealso=iteff]{ite}{ITE}{Individual Treatment Effects}
\newacronym[seealso=iteff]{dle}{DLE}{Drug Laboratory Effects}
\newacronym[seealso=pmed]{pm}{PM}{Personalized Medicine}
\newacronym{iot}{IoT}{Internet of Things}

%% Techniques
\newacronym{mba}{MbA}{\gls{mb-avg}}
\newacronym{bn}{BN}{batch-normalization}
\newacronym{sc}{SC}{shortcut connections}
\newacronym{cbt}{CBT}{Cluster-based training}
\newacronym{vcd}{VCD}{Variational contrastive divergence}
\newacronym{ln}{LN}{Layer normalisation}
\newacronym{ssl}{SSL}{Semi-supervised Learning}
\newacronym{sn}{SN}{Spectral Normalization}
\newacronym[seealso=self-training]{st}{ST}{Self-training}
\newacronym[seealso=co-training]{ct}{CT}{Co-training}

%% Privacy
\newacronym{dp}{DP}{Differential privacy}
\newacronym{dp-sgd}{DP-SGD}{Differential private stochastic gradient descent}
\newacronym{ad}{AD}{Attribute Disclosure}
\newacronym[seealso=mia]{pd}{PD}{Presence Disclosure}
\newacronym{rr}{RR}{Reproduction rate}
\newacronym[seealso=mia]{mi}{MI}{Membership Inference}

\newacronym{anm}{ANM}{Additive noise model}


%% Evaluation qualitative
\newacronym{ved}{VED}{Visual Expert Discrimination}

%% Evaluation statistics
\newacronym{dwpro}{DWS}{Dimension-wise Statistics}
\newacronym{dwpre}{DWP}{Dimension-wise Prediction}

%% Evaluation quantitative
\newacronym{fd}{FD}{Feature distributions}
\newacronym{qq}{QQ}{Quantile-quantile plot}
\newacronym{lsr}{LSR}{Latent space representation}
\newacronym{rdp}{RDP}{Renyi Differential Privacy}
\newacronym{pcam}{PCAM}{\gls{pca} Marginal}
\newacronym{pca}{PCA}{Principal Component Analysis}
\newacronym{pcawdd}{PCA-DWD}{\gls{pca} Distributinal Wassertein Distance}

%% Metrics
\newacronym{fop}{F-OP}{First-order proximity}
\newacronym{cc}{CC}{Correlation coefficient}
\newacronym{md-cc}{MD-CC}{\todo{Redundant?}}
\newacronym{mmd}{MMD}{Maximum Mean Discrepency}
\newacronym{rbf}{RBF}{Radial Basis Function}
\newacronym{mse}{MSE}{Mean Squared Error}
\newacronym{auroc}{AUROC}{Area under ROC curve}
\newacronym{auprc}{AUPRC}{Area under the precision-recall curve}
\newacronym{kld}{KLD}{Kullback-Leibler divergence}

%% Evaluation augmentation
\newacronym{tstr}{TSTR}{Train on synthetic, test on real}
\newacronym{trts}{TRTTS}{Train on real, test on synthetic}
\newacronym{pta}{PTA}{Prediction task accuracy}
\newacronym{ssa}{SSA}{Semi-supervised augmentation}




