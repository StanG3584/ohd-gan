\section{Discussion}
\subsection{Applications for GANs for health data and innovation}

Overall, on the aspect of data realism or fidelity, the published GAN algorithms for OHD provided equivalent or superior performance against the statistical modeling-based methods that many authors benchmarked against. Importantly, their demonstrated capabilities are highly relevant to the medical field: domain translation for unlabeled data, conditional sampling of minority classes, components of predictive models that promote generalization, learning from partially labeled or unlabeled data, data imputation, and forward simulation of patient profiles.

\subsection{Challenges posed by OHD}
On the other hand, the challenges posed by health data are obvious, and a number of recurrent factors influenced the outcome of efforts to develop GANs for OHD. We recognize the same challenges that were met by predictive machine learning, and continue to complexify the development and application of new algorithms. Whether aimed at generative or predictive algorithms, the complex integration of  heterogeneous information from different sources is further entangled with multimodality and missingness, among others.\par
In the case of generative models, multimodality is one aspect that  caused the most trouble in achieving a stable training procedure. At the outset, preventing mode collapse was an issue that attracted the most research efforts, along with data containing combinations of categorical and continuous features . It follows an extensive succession of efforts aimed at improving medGAN by incorporating the latest machine learning technique, known to improve performance across a broad range of applications. While a number of valuable improvements were demonstrated, taken as a whole the efforts were haphazard and often yielded unsurprising results. Clearly the opportunity for original techniques to considerably advance the field is still open, and more concerted efforts to systematically approach the problems could accelerate innovation.\par
While the problem of mode collapse has been alleviated, evidence has yet to be provided with regards to ensuring that the finer details of the distribution are estimated with sufficient granularity to produce realistic patient profiles. In this direction conditional training methods have led to improvements. For example, when labels corresponding to sub-populations or classes are used to condition the generative process. Zhang et al. showed that conditioned training with categorical labels, in this case age ranges, improves utility for small datasets, but not with larger samples \cite{Zhang2020-wp} As described in Section \ref{noauto}, HGAN further introduces constraint-based loss. Based on the distribution of individual features and utility-based metrics, the authors argue that the bias intrinsic to their methods has not led to undesirable bias or side-effects in other aspects of the learned distribution. The evaluation metrics put forward are insufficient to make such claims and caution should be advised in regards to techniques that constrain or direct the training procedure on specific sub-populations. Furthermore, this approach cannot practically account for every mode in all dimensions.

\subsection{Evaluation metrics and benchmarking}
In regards to the practices of evaluation, the choice of optimal metrics and indicators is still being explored. Overall, no evaluation metric proposed addresses the concept of realism in synthetic data. The blatant observation is that the efforts are far from consistent or systematic. This has led to a number of issues. As a striking example, competing methods are often compared with different metrics or with contradictory results in different datasets \cite{Baowaly2019,Baowaly_2019,Camino2018-re,Choi2017-nt,Zhang2020-wp}. In their evaluation of medGAN, Yale et al. argue that the positive resemblance of plotted feature distribution of synthetic data against real data is due to the fact that the model's architecture tends to favor reproducing the means and probabilities of each diagnosis column. For example, synthetic data contains samples with an unusually high number of codes. Their hypothesis is that these samples are used by the algorithm to discharge the rare medical codes with weak correlation to balance the distributions. However, they stated in their experiments that comparing PCA plots of real and synthetic data for various generation methods was insightful to get an impression of their behavior \cite{Yale_2020}.\par
Qualitative evaluation, in its current form, provides little evidence. For medical experts, these representations are meaningless. As such, the results of qualitative evaluation often state that synthetic data is indistinguishable from the real data \cite{choi2017generating,Wang_2019}. It is doubtful that they could in fact be. Esteban et al. found that participants avoided the median score and were not confident enough to choose either extreme (Esteban 2017).\par
Reproducing aggregate statistical properties is rather unconvincing evidence that a model has learned to reproduce the complexity of patient health trajectories. Choi et al. found that although the synthetic sample seemed statistically sound, it contained gross errors such as gender code mismatches and suggested the use of domain-specific heuristics \cite{choi2017generating}. HGAN was an encouraging step in this direction, but it may be difficult to scale. In some cases the statistical metrics may be contradictory, such as when the ranking of medical frequencies are wrong, but the data augmentation leads to improved performance \cite{Che_2017}
Utility-based metrics provide a more solid evaluation of data quality. However, these metrics only confirm the value of the data according to a narrow context. They are indicative of realism so far as a patient's state is indicative of a medical outcome. Moreover, they do not provide any insight about the validity of the relations found in a patient record and its overall consistency. 

\input{41-analysis}

\input{42-future}