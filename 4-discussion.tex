\section{Discussion}
\subsection{Applications of GANs for health data and innovation}

Overall, the published \gls{gan} algorithms for \gls{ohd} provided equivalent or superior performance against the statistical modeling-based methods against which they were benchmarked. Importantly, their capabilities are highly relevant to the medical field: domain translation for unlabeled data, conditional sampling of minority classes, data augmentation, learning from partially labeled or unlabeled data, data imputation, and forward simulation of patient profiles. While some of these claims are overoptimistic or lack convincing evidence, they paint an encouraging picture for the value of synthetic \gls{ohd} and the transformative effect it could have on healthcare initiatives and scientific progress. In this regard the replication of medical studies with synthetic data by \citeauthor{Yale_2020} substantiate the value of \gls{sd} for exploratory data analysis, reproducibility on restricted data and more generally education in scientific training are glaring. 
\begin{quote}
\textsc{}
\end{quote}

\begin{tcolorbox}[sharp corners, colback=teal!10, colframe=teal!80, title=Data utility]
More research efforts should by directed to demonstrating that synthetic data generated by GANs posses sufficient utility for scientific analyses. Reproducing published reslults of medical research is a straightforward and convincing way to acheive this. 
\end{tcolorbox}

\subsection{Challenges posed by OHD}
The challenges posed by health data are obvious, and a number of recurrent factors influenced the outcome of efforts to develop \glspl{gan} for \gls{ohd}. These problems are not limited to generative algorithms, but also \gls{ml} in general. While the progress in developing new algorithms has great momentum, their application and adoption will undoubtedly be more sluggish, as has been the case with predictive \gls{ml}.\par

In the case of generative models, multi-modality is one aspect that caused the most trouble in achieving a stable training procedure. At the outset, preventing mode collapse was an issue that attracted the most research efforts, in addition to data containing combinations of categorical and continuous features. A rapid succession of efforts aimed at improving \gls{medgan} by incorporating the latest machine learning techniques, known to improve performance across a broad range of applications, showed continued improvements. However, taken as a whole the efforts were haphazard and often yielded unsurprising results. This is not unexpected in a new field, and more concerted efforts to systematically approach the problems would surely formalize the research.\par

While the problem of mode collapse has been alleviated, evidence has yet to be provided with regards to ensuring that the finer details of the distribution are estimated with sufficient granularity to produce realistic patient profiles. In this direction conditional training methods have led to improvements. For example, when labels corresponding to sub-populations or classes are used to condition the generative process. \citeauthor{Zhang2020} showed that conditioned training with categorical labels, in this case age ranges, improves utility for small datasets, but not with larger samples \cite{Zhang2020} As described in Section \ref{noauto}, \gls{heterogan} further introduces constraint-based loss. Based on the distribution of individual features and utility-based metrics, the authors argue that the bias intrinsic to their methods has not led to undesirable bias or side-effects in other aspects of the learned distribution. 

The idea of introducing human knowledge in the otherwise naive training process has gained some attention. Not only can this improve the speed and quality of training, but also implies some degree of interpretability.

\subsection{Evaluation metrics and benchmarking}
In regards to the practices of evaluation, the choice of optimal metrics and indicators is still being explored. Overall, no evaluation metric proposed addresses the concept of realism in synthetic data. The blatant observation is that the efforts are far from consistent or systematic. This has led to a number of issues. As a striking example, competing methods are often compared with different metrics or with contradictory results in different datasets \cite{baowaly_2019_IEEE,baowaly_2019_jamia,Camino2018-re,Choi2017-nt,Zhang2020}. In their evaluation of \gls{medgan}, Yale et al. argue that the positive resemblance of plotted feature distribution of synthetic data against real data is due to the fact that the model's architecture tends to favor reproducing the means and probabilities of each diagnosis column. For example, synthetic data contains samples with an unusually high number of codes. Their hypothesis is that these samples are used by the algorithm to discharge the rare medical codes with weak correlation to balance the distributions. However, they stated in their experiments that comparing \gls{pca} plots of real and synthetic data for various generation methods was insightful to get an impression of their behavior \cite{Yale_2020}.\par
Qualitative evaluation, in its current form, provides little evidence. For medical experts, these representations are meaningless. As such, the results of qualitative evaluation often state that synthetic data is indistinguishable from the real data \cite{Choi2017-nt,Wang_2019}. It is doubtful that they could in fact be. Esteban et al. found that participants avoided the median score and were not confident enough to choose either extreme (Esteban 2017).\par
Reproducing aggregate statistical properties is rather unconvincing evidence that a model has learned to reproduce the complexity of patient health trajectories. Choi et al. found that although the synthetic sample seemed statistically sound, it contained gross errors such as gender code mismatches and suggested the use of domain-specific heuristics \cite{Choi2017-nt}. \gls{heterogan} was an encouraging step in this direction, but it may be difficult to scale. In some cases the statistical metrics may be contradictory, such as when the ranking of medical frequencies are wrong, but the data augmentation leads to improved performance \cite{Che_2017}
Utility-based metrics provide a more solid evaluation of data quality. However, these metrics only confirm the value of the data according to a narrow context. They are indicative of realism so far as a patient's state is indicative of a medical outcome. Moreover, they do not provide any insight about the validity of the relations found in a patient record and its overall consistency. 

\subsection{Analysis of OHD-GAN}
\subsubsection{Data representation and algorithm architecture}
We observed that majority of methods included in the review made use of  altered representations of patient records. Namely, through feature engineering the data is transformed from its original form. This is in part due to the inconvenient properties of health data, such as missingness. However, it is somewhat apparent that the main motive is to accommodate existing algorithms. Along with demographic variables, \gls{ohd} data mostly takes the form of triples composed by a timestamp, a medical concept and the recorded value. Their count is different for each patient, irregular intervals between each triple and the number of possible values in a dimensions can be huge. Moreover, there are generally multiple episodes of care, each with a different cause. The form and content is not typically considered practical for machine learning. \par
At varying degrees, depending on the transformations, information is being lost or bias is being  introduced. For example, when data are reduced by aggregation to one-hot encoding of binary or count variables, the complex relationships found in medical data are, for the most part, lost. Similarly, information is lost when forcing continuous time-series into a regular representation, by truncating, padding, binning or imputation. Moreover, it is highly unlikely that the data is missing at random, introducing the potential for bias when a large part of the real data is rejected on this basis. Truncating the medical codes to their parent generalizations \cite{Zhang2020, Choi2017-nt}.  In brief, loss of information content is being preferred by molding and discarding arbitrarily the data to the benefit of performance metrics, as opposed to the more tricky alternative of developing algorithms according the data.\par
Deep architectures are based on the intuition that multiple layers of nonlinear functions are needed to learn complicated high-level abstractions \cite{Bengio_2009}. CNN capture patterns of an image in a hierarchical fashion, such that in sequence, each layer forms a representation the data at a higher level of abstraction. This type of data-oriented architecture has led to impressive performance for CNN and image data. The same principle can be applied to health data. An algorithm developed in a hierarchical structure, was demonstrated to form representations of \gls{ehr} that capture the sequential order of visits and co-occurrence of codes in a visit have led to improved predictor performance, and also allowed for meaningful interpretation of the model \cite{choi2016multi}. Similarly, models of time-series based on a continuous time representation, such as found in \gls{ehr} data, have shown improved accuracy over discrete time-representations \cite{rubanova2019latent,de2019gru}. Nonetheless, creative adaptations of the data for existing architectures have provided surprising results. For example, \gls{ohd} input into a CNN were transformed to image(bitmaps) in which the pixels encoded the information \cite{Fukae2020}.

\section{Recommendations}\label{sec:recommend}
\subsection{Basic models}\label{sec:basic}

Overall, evaluation methods were superficial or uni-dimensional. Finding convincing and robust evaluation metrics for synthetic health data is an open issue. Even more so when the learning task is poorly defined or the scope of the problem is too large. The difficulty of explaining or validating the realism of data representing a patient, often longitudinal and which factors deferentially contribute to disease characterization makes the assessment of synthetic data ambiguous, thus demanding stronger evidence to claims.\par
Modelling efforts for \gls{ohd}-GAN should be limited in scope to a single data type or modality. This is favourable for a number of evaluation related aspects. Firstly, it makes qualitative evaluation by visual inspection from experts possible and meaningful. Secondly, for same reasons, the behaviour of the model can be assessed straightforwardly. The generative process can be influenced intentionally to observe the effect on the properties of the output. Finally, it allows for quantitative evaluation with domain specific metrics. The scope should clearly identify the purpose of the data generation, its utility and the target patients\cite{Capobianco2020,Kappen_2016, Kappen_2016a}

\subsection{Data-driven architecture}\label{sec:archi}
The algorithm architecture of \gls{ohd}-GAN should be engineered to match the process that generated the data, not the other way around. Data should be used and generated in the form it is first collected. In addition to preventing information loss, this ensures models will reflect the real generative process. Such models are more likely to provide insights into the system they are taught to imitate and further our understanding about them. Furthermore, the learned statistical distribution is inevitably more meaningful and interpretable, facilitating applications in the healthcare domain and supporting the inference of insights from the learned model parameters.
\subsection{Interpretability}
Even though a few authors explored the behavior of their models according to various methods, the subject was left largely unmentioned. It is imperative that future experimentation and publication give equal importance to evaluating the interpretation of their models and means to do so, as for performance. In the healthcare domain, black box machine learning models find little adoption, and synthetic data is most often met with attacks to its validity.

\section{Directions for future research}
\subsection{Building a patient model}
The ultimate goal for generative models of \gls{ohd} must be to develop an algorithm capable of learning an all encompassing patient model. It would then be possible to generate full \gls{ehr} records on demand, integrating genetic, lifestyle, environmental, biochemical, imaging, clinical information into high-resolution patient profiles \cite{Capobianco2020}. This is in fact the intention of the patient simulator Synthea. However, Synthea will eventually face a problem with scalability and the capacity of semi-independent state-transition models to coordinate in capturing long-range correlations.\par

Once basic models of health data, as described in Section \ref{sec:basic}, have been developed and validated, these can be progressively combined in a modular fashion to obtain increasingly complex patient simulators. Furthermore, having designed the architecture of these basic models on the underlying data in a way that is comprehensible, as described in \ref{sec:archi}, will facilitate the composition of more complex models. Inputs, outputs and parts of these models can be conditionally attached to others such that the generative process occurs in a way that reflects the real generative process.

\subsection{Evaluating complex patient models \label{sec:evaluation-cqm}}
Once more complex models are developed, the problem is again finding meaningful evaluation metrics of data realism. Capobiano et al. insist on the necessity for data performance metrics encompassing diagnostic accuracy, early intervention, targeted treatment and drug efficacy \cite{Capobianco2020}. In their publication exploring the validation of the data produced by Synthea, Chen et al. provide an interesting idea to achieve this \cite{Chen_2019}. Noting that the quality of care is the prime objective of a functional healthcare system, they suggest using \glspl{cqm} to evaluate the synthetic data. These measures "are evidence-based metrics to quantify the processes and outcomes of healthcare", such as "the level of effectiveness, safety and timeliness of the services that a healthcare provider or organization offers."(Chen 2019). High-level indicators such as \glspl{cqm} domain specific measures of quality, specifically designed for higher level or multi-modal representations of healthcare data. The constraints introduced in \gls{heterogan} should be leverage to evaluate the realism of the synthetic data, rather than bias the generator training. Composing a comprehensive set of such constraints could possibly serve as a standardized benchmark.
At the individual level, Walsh et al. employ domain specific indicators of disease progression and worsening and compare agreement of the simulated patient trajectories with the factual timelines \cite{walsh2020generating}.\par
In addition to \gls{cqm}, we propose the use of the Care maps used by the Synthea model to simulate patient trajectories as evaluation metrics \cite{Walonoski_2017}. Care maps are transition graphs developed from clinician input and Clinical Practice Guidelines, of which the transition probabilities are gathered from health incidence statistics. While these allow the Synthea algorithm to simulate patient profile with realistic structure, they also prevent it from reproducing real-world variability. Conversely, while \glspl{gan} have the ability to reproduce the quirks of real data, they also lack the constraints preventing nonsensical outputs. As such, Care maps provide an ideal metric to check if the synthetic data conforms to medical processes.\par 
In fact, has been used before in a competition where participants were given synthetic data from finite state transition machines with know probabilities and tasked to build and learn models that would reproduce those of the original, unseen models. The participants according to the Perplexity metric. Commonly used in NLP, quantifies how well a probability distribution or probability model predicts a sample \cite{Verwer_2013}. We postulate that the Synthea models built with real-world probabilities would provide a unique and robust way to evaluate synthetic data according to the metric proposed above, among other means to utilize the state-transition in Synthea and their modularity.

\subsubsection{Latent space \label{sec:latent-space}}
The latent space representation, the lower-dimensional vector space of the data, can provide means for evaluation and interpretability and it's potential should be explored when developing algorithms \cite{lui2019-latent}. Numerous publications have shown that they capture meaningful properties and structure of the data, reducing complexity to a level that lends itself to interpretation \cite{Way2020, Koumakis2020}. In one instance involving transcription factor micro-array data, a close one-to-one mapping could be obtained from the last hidden layer, in addition to the higher level layers that related to biological processes in a hierarchical fashion \cite{chen2016-latentyeast}. Pushing the boundaries further, by correlating the output features of a GAN with the latent space dimensions allowed controllable semantic manipulation of the generated data \cite{Wang2020latent,Ding2020latent,Li2020latent}, or provided new insights by exploring structured perturbations \cite{lui2019-latent}.

\subsubsection{Opportunities and application to current events}
Synthetic and external controls in clinical trials are becoming increasingly popular \cite{Thorlund2020}. Synthetic controls refer to cohorts that have been composed from real observational cohorts or \gls{ehr} using statistical methodologies. While the individuals included in the cohorts are usually left unchanged, micro-simulations of disease progression at the patient level are used to explore long-term outcomes and help in the estimation of treatment effects \cite{Thorlund2020, Etzioni2002}. Synthetic data generated by \glspl{gan} could be transformative for the problem of finding control cohorts.\par
With the COVID-19 pandemic scientists have become increasingly aware of and vocal about the need for data sharing between political borders \cite{Cosgriff_2020,Becker_2020,McLennan_2020}. An obvious application is generating additional amounts of data in the early stages of the pandemic, potentially creating opportunities earlier. Synthetic is data not only an opportunity to facilitate the exchange of data, but also adjust the biases of samples obtained from different localities. Factors such as local hospital practices, different patient populations and equipment introduce feature and distribution mismatches \cite{Ghassemi2020}. These disparities can be mitigated by translation of \gls{gan} algorithms, such as \gls{cycle-gan} proposed by Yoon et al.