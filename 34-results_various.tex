\subsubsection{Forgoing the autoencoder}\label{noauto}

Suggesting the use of an \gls{ae} introduces noise, with \gls{emr-wgan}, Zhang et al. dispose of the \gls{ae} component of previous algorithms and introduce a conditional training method, along with conditioned \gls{bn} and \gls{ln} techniques to stabilise training \cite{Zhang2020-wp}. The algorithm was further adapted by Yan et al. as \gls{heterogan} to better account for the conditional distributions between multiple data types and enforce record-wise consistency. A recognized problem with \algo{medgan} was that it produced common-sense inconsistencies, such as gender mismatches in medical codes \cite{yan2020generating, choi2017generating}. In \gls{heterogan}, constraints are enforced by adding specific penalties to the loss function, such as limit ranges for numerical categorical pairs and mutual exclusivity for pairs of binary features \cite{yan2020generating}. \par

To develop \gls{ctgan}, Xu et al. presume that tabular data poses a challenge to GANs owing to the non-Gaussian multi-modal distribution of continuous columns and imbalanced discrete columns \cite{Xu2019-ay}. Their algorithm, composed of fully connected layers, was developed with adaptations to deal with both continuous and categorical features. For continuous features, it employs mode-specific normalization to capture the multiplicity of modes. For discrete features conditional training-by sampling is devised to resample discrete attributes evenly during training, while recovering the real distribution when generating data.\par

Other approaches include: \gls{corgan}, where the \gls{ae} is questionably replaced by a \gls{1d-cae} to capture neighboring feature correlations of the input vectors \cite{torfi2019generating}, and two basic feedforward networks based on Wassertein distance to evaluate the capacity of \glspl{gan} to model heterogeneous data of dense and sparse medical features \cite{chincheong2020generation} and to reproduce statistical properties \cite{ozyigit2020generation}. Reproducing physiological time-series \citeauthor{esteban2017real} used devise the \gls{rgan} and \gls{rcgan} based on \gls{lstm} to generate a regular time-series of physiological measurements from bedside monitors \cite{esteban2017real}. Curiously, the authors dismiss Wassertein's distance, stating that they did not find application in their experiments. In addition, each dimension of their time-series is generated independently from the others, where one would assume they are correlated. A considerable loss of accuracy is observed on their \gls{utility-metric}.

\subsection{Task oriented GAN development}
\subsubsection{Semi-supervised learning and conditional models}

To develop \gls{ehrgan}, an algorithm for sequences of medical codes that has the ability to produce neighbouring records of an input patient, \citeauthor{Che_2017} combine an Encoder-Decoder \gls{cnn} \cite{ranzato2007unsupervised} with \gls{vcd} \cite{Che_2017}. The \gls{ehrgan} generator is trained to decode a random vector mixed with the latent space representation of a particular patient. In a semi-supervised learning approach, the trained \gls{ehrgan} model is then incorporated into the loss function of a predictor where it can help generalization by producing neighbors for each input sample. Semi-supervised learning approaches are commonly employed to augment the minority class in imbalanced datasets, such as \gls{self-training} and \gls{co-training}.  Yang et al. improve on this type of approach by incorporating a GAN in the procedure \cite{yang}. The GAN is first trained on the labelled set and used to rebalance it. The standard iterative process involving the classifier ensemble is then executed until expansion ceases. As a final step, the GAN is trained on the expanded labelled set to generate an equal amount of augmentation data. The authors obtained improved performance in a number of classification tasks and multiple tablular datasets with their method.Correcting bias with domain translationTo address the heteogeneity of healthcare data from different sources, Yoon et al. combines the concepts of cycle-consistent domain translation from Cycle-GAN (Zhu 2017) and multi-domain translation from Star-GAN (Choi 2017a) to build RadialGAN to translate heterogeneous patient information from different hospitals, correcting features and distribution mismatches (Yoon 2018). An encoder-decoder pair per data endpoint is trained to map records to and from a shared latent representation. Individualized treatment effectsThe task of estimating Individualized Treatment Effects (ITE), the response of a patient to a certain treatment given a set of charaterizing features is an ongoing problem. This is due mainly to the fact that counterfactual outcomes are never observed or that treatment selection is highly biased (Yoon 2018a, McDermott 2018, Walsh 2020). In this regard, Yoon et al. employ a pair of GANs, named Generative Adversarial Nets for inference of Individualized Treatment Effects (GANITE), one for counterfactual imputation and another for ITE estimation (Yoon 2018a). The former captures the uncertainty in unobserved outcomes by generating a variety of conterfactuals. The output is fed to the latter, which estimates treatment effects and provides confidence intervals. With Cycle Wasserstein Regression GAN (CWR-GAN), a joint regression-adversarial model, McDermott et al. demonstrated a semi-supervised approach also inspired by Cycle-GAN to leverage large amounts of unpaired pre/post-treatment time-series in ICU data for the estimation of ITE on physiological time-series (McDermott 2018). The algorithm has the ability to learn from unpaired samples, with very few paired samples, to reversibly translate the pre and post-treatment physiological series. Chu et al. approach the problem of data scarcity for ITEs by designing ADTEP, an algorithm that can maximize use of the large volume of EHR data formed by triples of non-task specific patient features, treatment interventions and treatment outcomes (Chu 2019). The ADTEP algorithm they developed learns representation and discriminatory features of the patient, and treatment data by training an \gls{ae} for each pair of features. In addition to \gls{ae} reconstruction loss, a second model is tasked with identifying fake treatment feature reconstructions. Finally, a fourth loss metric is calculated by feeding the concatenated latent representations of both \gls{ae} to a logisitic regression model aimed at predicting the treatment outcome (Chu 2019). In the form of an ITE task, Wang et al. demonstrated an interesting algorithm to generate a time series of patient states and medication dosages using \gls{lstm}. In contrast to RGAN and RCGAN, in Sequentially Coupled Generative Adversarial Network (SC-GAN), patients state at the current timestep informs the concurrent medication dosage, which in turn affects the patient state in the upcoming timestep (Wang 2019). SC-GAN overcame a number of baselines on both statistical and utility metrics. Data Imputation with GANsGANs are naturally suited for data imputation, and could provide a new approach to deal with the problems of health data relating to sparsity. Statistical models developed for the multiple imputation problem increase quadraticly in complexity with the number of features, while the expressiveness of deep neural networks can model all features with missing values simultaneously efficiently. In that regard, Yoon et al. adapted the standard GAN to perform imputations on continuous features missing at random in tabular datasets (Yoon 2018b). In their algorithm GAIN, the discriminator is tasked with classifying individual variables as real or fake (imputed), as opposed to the whole ensemble. Additional input, or hint, containing the probability of each component being real or imputed is fed to the discriminator to resolve the multiplicity of optimal distributions that the generator could reproduce. The model performs considerably better than five state-of-the-art benchmarks. The GAIN algorithm was later adapted to also handle categorical features using fuzzy binary encoding, the same technique employed in HealthGAN (Yale 2019)Data augmentationThe distribution estimated by a generator model can compensate for lack of diversity in a real sample, essentially filling in the blanks in a manner comparable to data imputation. In such cases, data sampled from this distribution has the potential to help improve generalization in training predictive models. We find evidence of this by way of generating unobserved counterfactual outcomes (Yoon 2018a), or generating neighboring samples to help generalization in predictors (Che 2017). The RBM developed by Fisher et al. enabled them to simulate individualized patient trajectories based on their base state characteristics. Due to the stochastic nature of the algorithm, generating a large number of trajectories for a single patient can provide new insights of the influence of starting conditions on disease progression or quantify risk (Fisher 2019).