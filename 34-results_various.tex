\subsubsection{Forgoing the autoencoder}\label{noauto}

With EMR Wasserstein GAN (EMR-WGAN), Zhang et al. dispose of the AE component in medGAN and introduce a conditional training method, with along conditioned batch normalization and layer normalization techniques to stabilise training (Zhang 2020). The algorithm was further adapted by Yan et al. as Heterogeneous GAN (HGAN) to better account for the conditional distributions between multiple data types and enforce record-wise consistency. A recognized problem with medGAN was that it produced common-sense inconsistencies, such as gender mismatches in medical codes (Yan 2020, Choi 2017). In HGAN, constraints are enforced by adding specific penalities to the loss function, such as ranges for numerical categorical pairs and mutual exclusivity for pairs of binary features (Yan 2020). To develop Conditional Tabular GAN (CTGAN), Xu et al. presume that tabular data poses a challenge to GANs owing to the non-Gaussian multimodal distribution of continuous columns and imbalanced discrete columns. Their algorithm, composed of fully connected layers, was developed with adaptations to deal with both continuous and categorical features. For continuous features, it employs mode-specific normalization to capture the multiplicity of modes. For discrete features conditional training-by sampling is devised to resample discrete attributes evenly during training, while recovering the real distribution when generating data (Xu 2019). Other approaches include: corGAN, where the AE is questionably replaced by a 1-dimensional Convolutional AE (CAE) to capture neighboring feature correlations of the input vectors (Torfi 2019), and two basic feedforward networks based on Wassertein distance to evaluate the capacity of GANs to model heterogeneous data of dense and sparse medical features (Chin-Cheong 2019) and to reproduce statistical properties (Ozyigit 2020). Reproducing physiological time-series Esteban et al. used devise the Recurrent GAN (RGAN) and Recurrent Conditional GAN (RCGAN) based on LSTM to generate a regular time-series of physiological measurements from bedside monitors (Esteban 2017). Curiously, the authors dismiss Wassertein's distance, stating that they did not find application in their experiments. In addition, each dimension of their time-series is generated independently from the others, where one would assume they are correlated. A considerable loss of accuracy is observed on their utility based metric.Task oriented GAN developmentSemi-supervised learning and conditional modelsTo develop ehrGAN, an algorithm for sequences of medical codes that has the ability to produce neighbouring records of an input patient, Che et al. combine an Encoder-Decoder Convolutional Neural Network (CNN) (Ranzato 2007) with Variational Contrastive Divergence (VCD) (Che 2017). The ehrGAN generator is trained to decode a random vector mixed with the latent space representation of a particular patient. In a semi-supervised learning approach, the trained ehrGAN model is then incorporated into the loss function of a predictor where it can help generalization by producing neighbors for each input sample. Semi-supervised learning approaches are commonly employed to augment the minority class in imbalanced datasets. The self-training and co-training methods use classifiers first trained on the portion of labelled data to predict the labels of unlabelled instances. The newly labelled samples with the highest confidence are added to the labelled set to retrain the classifiers. The process is repeated iteratively. Yang et al. improve on this type of approach by incorporating a GAN in the procedure (Yang 2019). The GAN is first trained on the labelled set and used to rebalance it. The standard iterative process involving the classifier ensemble is then executed until expansion ceases. As a final step, the GAN is trained on the expanded labelled set to generate an equal amount of augmentation data. The authors obtained improved performance in a number of classification tasks and multiple tablular datasets with their method.Correcting bias with domain translationTo address the heteogeneity of healthcare data from different sources, Yoon et al. combines the concepts of cycle-consistent domain translation from Cycle-GAN (Zhu 2017) and multi-domain translation from Star-GAN (Choi 2017a) to build RadialGAN to translate heterogeneous patient information from different hospitals, correcting features and distribution mismatches (Yoon 2018). An encoder-decoder pair per data endpoint is trained to map records to and from a shared latent representation. Individualized treatment effectsThe task of estimating Individualized Treatment Effects (ITE), the response of a patient to a certain treatment given a set of charaterizing features is an ongoing problem. This is due mainly to the fact that counterfactual outcomes are never observed or that treatment selection is highly biased (Yoon 2018a, McDermott 2018, Walsh 2020). In this regard, Yoon et al. employ a pair of GANs, named Generative Adversarial Nets for inference of Individualized Treatment Effects (GANITE), one for counterfactual imputation and another for ITE estimation (Yoon 2018a). The former captures the uncertainty in unobserved outcomes by generating a variety of conterfactuals. The output is fed to the latter, which estimates treatment effects and provides confidence intervals. With Cycle Wasserstein Regression GAN (CWR-GAN), a joint regression-adversarial model, McDermott et al. demonstrated a semi-supervised approach also inspired by Cycle-GAN to leverage large amounts of unpaired pre/post-treatment time-series in ICU data for the estimation of ITE on physiological time-series (McDermott 2018). The algorithm has the ability to learn from unpaired samples, with very few paired samples, to reversibly translate the pre and post-treatment physiological series. Chu et al. approach the problem of data scarcity for ITEs by designing ADTEP, an algorithm that can maximize use of the large volume of EHR data formed by triples of non-task specific patient features, treatment interventions and treatment outcomes (Chu 2019). The ADTEP algorithm they developed learns representation and discriminatory features of the patient, and treatment data by training an AE for each pair of features. In addition to AE reconstruction loss, a second model is tasked with identifying fake treatment feature reconstructions. Finally, a fourth loss metric is calculated by feeding the concatenated latent representations of both AE to a logisitic regression model aimed at predicting the treatment outcome (Chu 2019). In the form of an ITE task, Wang et al. demonstrated an interesting algorithm to generate a time series of patient states and medication dosages using LSTM. In contrast to RGAN and RCGAN, in Sequentially Coupled Generative Adversarial Network (SC-GAN), patients state at the current timestep informs the concurrent medication dosage, which in turn affects the patient state in the upcoming timestep (Wang 2019). SC-GAN overcame a number of baselines on both statistical and utility metrics. Data Imputation with GANsGANs are naturally suited for data imputation, and could provide a new approach to deal with the problems of health data relating to sparsity. Statistical models developed for the multiple imputation problem increase quadraticly in complexity with the number of features, while the expressiveness of deep neural networks can model all features with missing values simultaneously efficiently. In that regard, Yoon et al. adapted the standard GAN to perform imputations on continuous features missing at random in tabular datasets (Yoon 2018b). In their algorithm GAIN, the discriminator is tasked with classifying individual variables as real or fake (imputed), as opposed to the whole ensemble. Additional input, or hint, containing the probability of each component being real or imputed is fed to the discriminator to resolve the multiplicity of optimal distributions that the generator could reproduce. The model performs considerably better than five state-of-the-art benchmarks. The GAIN algorithm was later adapted to also handle categorical features using fuzzy binary encoding, the same technique employed in HealthGAN (Yale 2019)Data augmentationThe distribution estimated by a generator model can compensate for lack of diversity in a real sample, essentially filling in the blanks in a manner comparable to data imputation. In such cases, data sampled from this distribution has the potential to help improve generalization in training predictive models. We find evidence of this by way of generating unobserved counterfactual outcomes (Yoon 2018a), or generating neighboring samples to help generalization in predictors (Che 2017). The RBM developed by Fisher et al. enabled them to simulate individualized patient trajectories based on their base state characteristics. Due to the stochastic nature of the algorithm, generating a large number of trajectories for a single patient can provide new insights of the influence of starting conditions on disease progression or quantify risk (Fisher 2019).